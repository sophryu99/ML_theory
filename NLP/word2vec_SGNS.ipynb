{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_SGNS.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOocW9sJjMae9cSvX6xw9Fw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophryu99/machine-learning/blob/main/word2vec_SGNS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIs-qm5xzZXc"
      },
      "source": [
        "# word2vec - skip-gram with negative sampling, SGNS\t\n",
        "### **Skip-gram**\n",
        "**Skip-gram**: 중심 단어로부터 주변 단어를 예측하는 모델 \\\n",
        "<img width=\"250\" alt=\"Screen Shot 2021-05-08 at 3 25 24 PM\" src=\"https://user-images.githubusercontent.com/46921003/117529256-9ecb7a80-b011-11eb-913c-a63d91643057.png\"> <br/>\n",
        "> 중심단어 cat을 사용해서 주변단어 the, fat, sat, on을 예측한다.\n",
        "\n",
        "<br/>\n",
        "\n",
        "### **Skip-gram with Negative Sampling, SGNS**: \n",
        "\n",
        "skip-gram의 문제점: 학습 과정에서는 '나무'에서 '열매'라는 단어를 예측할 때 매우 무관한 단어들(ex. 인형, 노트북)까지에 대해서도 학습을 해야하는 상황이 있다. 이러한 \"**무관한 단어들에 대해서는 weight를 업데이트하지 않아도 된다**\"가 negative sampling의 기본 아이디어이다.\n",
        "\n",
        "Negative sampling은 Target이 negative인 단어들을 샘플링하는 과정이다. \n",
        "위의 인형이나 노트북과 같은 무관한 단어는 target 값이 0인 negative 값이고 \"열매\"는 target 값이 1인 positive 값이다.\n",
        " \n",
        "중심 단어와 주변 단어가 모두 입력이 되고, 이 두 단어가 실제로 윈도우 크기 내에 존재하는 이웃 관계인지 그 확률을 예측 <br/>\n",
        "<img width=\"250\" alt=\"Screen Shot 2021-05-08 at 3 34 50 PM\" src=\"https://user-images.githubusercontent.com/46921003/117529516-ef8fa300-b012-11eb-8e70-045e65ebfedb.png\"> <img width=\"213\" alt=\"Screen Shot 2021-05-10 at 2 30 52 PM\" src=\"https://user-images.githubusercontent.com/46921003/117610221-5391a300-b19c-11eb-946d-1c628da41f94.png\">\n",
        "<br/>\n",
        "\n",
        "- n개의 negative 값을 선택하고 이 값들에 대해서만 positive 값과 함께 학습한다. (n은 사용자설정)\n",
        "- wi 는 i번째 단어이고, f(wi)는 해당 단어의 빈도, (출현횟수/전체corpus수) 이다. \n",
        "- P(wi)는 wi가 나타날 확률 \n",
        "\n",
        "기존의 **skip-gram 데이터셋**을 **SGNS의 데이터셋**으로 바꾸는 과정: <br/>\n",
        "<img width=\"500\" alt=\"Screen Shot 2021-05-08 at 3 37 10 PM\" src=\"https://user-images.githubusercontent.com/46921003/117529585-4301f100-b013-11eb-87b0-c9938a4d4a1c.png\"> <br/>\n",
        "\n",
        "- 좌측의 테이블: 기존의 skip-gram을 학습하기 위한 데이터셋\n",
        "  - 중심 단어를 입력, 주변 단어를 레이블로 설정\n",
        "- 우측의 테이블: SGNS를 학습하기 위해서는 해당 테이블과 같이 수정해야함\n",
        "  - 왼쪽의 데이터셋에서 중심단어와 주변단어를 각각 입력1, 입력2로 받아줌\n",
        "  - 윈도우 내에서 이웃 관계였기에 레이블을 1로 설정해줌\n",
        "  - 이웃관계가 아닌 단어(입력2)는 레이블을 0으로 설정해줌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2-WwsVZ5rlf"
      },
      "source": [
        "## **1. SGNS 구현**\n",
        "\n",
        "1. **두개의 임베딩 테이블 생성** <br/>\n",
        "<img width=\"691\" alt=\"Screen Shot 2021-05-08 at 3 56 07 PM\" src=\"https://user-images.githubusercontent.com/46921003/117530027-e94ef600-b015-11eb-92f2-05b2b3a3961a.png\">\n",
        "- 좌측의 테이블: 중심단어(입력1)의 테이블 룩업을 위한 임베딩 테이블\n",
        "- 우측의 테이블: 주변단어(입력2)의 테이블 룩업을 위한 임베딩 테이블\n",
        "\n",
        "2. **중심단어와 주변단어의 임베딩 벡터값을 업데이트** <br/>\n",
        "\n",
        "<img width=\"669\" alt=\"Screen Shot 2021-05-08 at 3 58 39 PM\" src=\"https://user-images.githubusercontent.com/46921003/117530101-421e8e80-b016-11eb-8b10-aa85f750f8a1.png\"> <br/>\n",
        "\n",
        "- 중심 단어와 주변단어의 내적값 `(L = log sigmoid (Xw.T * θv) + ∑neg(v) [log sigmoid (-Xw.T * θneg(v))]`을 이 모델의 예측값으로 하고, **레이블과의 오차로부터 역전파**하여 **중심 단어**와 **주변 단어**의 **임베딩 벡터값을 업데이트** 한다. 이후에는 선택적으로 두 개의 임베딩 테이블 중 좌측의 테이블을 최종 임베딩 테이블로 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eyBiDI0z6YE",
        "outputId": "f065eaaa-8274-4e82-ca66-71f234187a96"
      },
      "source": [
        "# SGNS Model Implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "  \"\"\" 1.두개의 임베딩 테이블 생성, 및 임베딩 사이즈/디멘션 설정 \"\"\"\n",
        "  def __init__(self, emb_size, emb_dimension):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.w_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)  # 중심 단어용 임베딩 테이블\n",
        "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)  # 주변 단어용 임베딩 테이블\n",
        "        self._init_emb()\n",
        "\n",
        "  def _init_emb(self):\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
        "\n",
        "  \"\"\" 2. 중심단어와 주변단어의 임베딩 벡터값을 업데이트\n",
        "  내적의 결과는 1 또는 0을 예측하기 위해서 시그모이드 함수를 활성화 함수로 거쳐 최종 예측값을 얻는다.\n",
        "  L = log sigmoid (Xw.T * θv) + ∑neg(v) [log sigmoid (-Xw.T * θneg(v))]\n",
        "  \"\"\"\n",
        "  def forward(self, pos_w, pos_v, neg_v):\n",
        "        emb_w = self.w_embeddings(torch.LongTensor(pos_w))  # tensor로 convert: [ mini_batch_size * emb_dimension ]\n",
        "        emb_v = self.v_embeddings(torch.LongTensor(pos_v))\n",
        "        neg_emb_v = self.v_embeddings(torch.LongTensor(neg_v))  # 변환된 크기: [ negative_sampling_number * mini_batch_size * emb_dimension ]\n",
        "        score = torch.mul(emb_w, emb_v).squeeze()\n",
        "        score = torch.sum(score, dim=1)\n",
        "        score = F.logsigmoid(score)\n",
        "        neg_score = torch.bmm(neg_emb_v, emb_v.unsqueeze(2)).squeeze()\n",
        "        neg_score = F.logsigmoid(-1 * neg_score) # L = log sigmoid (Xw.T * θv) + ∑neg(v) [log sigmoid (-Xw.T * θneg(v))]\n",
        "        print(\"Negative_score is:\", neg_score[0][0].item())\n",
        "        loss = -1 * (torch.sum(score) + torch.sum(neg_score))\n",
        "        print(\"Loss is\", loss.item())\n",
        "        return loss\n",
        "\n",
        "  \"\"\" 3. 임베딩 결과 저장 \"\"\"\n",
        "  def save_embedding(self, id2word, file_name):\n",
        "        embedding = self.w_embeddings.weight.data.numpy()\n",
        "        fout = open(file_name, 'w')\n",
        "        fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "        for wid, w in id2word.items():\n",
        "            e = embedding[wid]\n",
        "            e = ' '.join(map(lambda x: str(x), e))\n",
        "            fout.write('%s %s\\n' % (w, e))\n",
        "\n",
        "def test():\n",
        "    model = SkipGramModel(100, 10)\n",
        "    id2word = dict()\n",
        "    for i in range(100):\n",
        "        id2word[i] = str(i)\n",
        "    pos_w = [0, 0, 1, 1, 1]\n",
        "    pos_v = [1, 2, 0, 2, 3]\n",
        "    neg_v = [[23, 42, 32], [32, 24, 53], [32, 24, 53], [32, 24, 53], [32, 24, 53]]\n",
        "    model.forward(pos_w, pos_v, neg_v)\n",
        "    model.save_embedding(id2word, 'test.txt')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative_score is: -0.6931471824645996\n",
            "Loss is 13.862945556640625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vq-jdPL1xNd"
      },
      "source": [
        "## **2. SGNS Input Data 형식**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0hVRFeV9JCE",
        "outputId": "a2254360-1b4d-4eeb-d0b9-87b758cee82d"
      },
      "source": [
        "# Data 준비\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "print('총 샘플 수 :',len(documents))\n",
        "\n",
        "# 불필요한 토큰 제거 및 소문자화를 통한 정규화\n",
        "news_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
        "# Null 값 처리\n",
        "news_df.dropna(inplace=True)\n",
        "\n",
        "# 불용어를 제거\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "tokenized_doc = tokenized_doc.to_list()\n",
        "\n",
        "# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\n",
        "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
        "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
        "print('총 샘플 수 :',len(tokenized_doc))\n",
        "\n",
        "# 단어 집합을 생성하고, 정수 인코딩 진행\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_doc)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "총 샘플 수 : 11314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "총 샘플 수 : 10940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "-xJyQ9Gs9WYK",
        "outputId": "bebae7c6-ddfe-4885-ef0f-904397b035a0"
      },
      "source": [
        "#news_df['clean_doc'].to_csv(sep=' ', index=False, header=False)\n",
        "#news_df['clean_doc'].to_csv('/pandas.txt', header=None, index=None, sep='\\t', mode='a')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-06dc68f5b710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'daaaata'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD-aUVLL0jR2"
      },
      "source": [
        "# Input data formatting\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class InputData:\n",
        "    def __init__(self, input_file_name, min_count):\n",
        "        self.input_file_name = input_file_name\n",
        "        self.input_file = open(self.input_file_name)  # 데이터 불러오기\n",
        "        self.min_count = min_count  # 제거할 단어의 최소 빈도수\n",
        "        self.wordId_frequency_dict = dict()  # 단어 ID- 빈도수 dict\n",
        "        self.word_count = 0  # 단어 수 (반복되는 단어는 1로만 계산)\n",
        "        self.word_count_sum = 0  # 총 단어 수 (반복되는 단어 수도 누적 됨)\n",
        "        self.sentence_count = 0  # 문장 수\n",
        "        self.id2word_dict = dict()  # 단어 ID - 단어 dict\n",
        "        self.word2id_dict = dict()  # 단어 ID dict\n",
        "        self._init_dict()  # dict 초기화\n",
        "        self.sample_table = []\n",
        "        self._init_sample_table()  # sample table 초기화\n",
        "        self.word_pairs_queue = deque() # deque에 pair 저장\n",
        "        # Training 진행 시 아웃풋\n",
        "        print('Word Count is:', self.word_count)\n",
        "        print('Word Count Sum is', self.word_count_sum)\n",
        "        print('Sentence Count is:', self.sentence_count)\n",
        "        print('Word pairs deque is:', self.id2word_dict)\n",
        "\n",
        "    def _init_dict(self):\n",
        "        word_freq = dict()\n",
        "        # word_frequency\n",
        "        for line in self.input_file:\n",
        "            line = line.strip().split(' ')  # 공백 제거\n",
        "            self.word_count_sum += len(line)\n",
        "            self.sentence_count += 1\n",
        "            for word in line:\n",
        "                try:\n",
        "                    word_freq[word] += 1\n",
        "                except:\n",
        "                    word_freq[word] = 1\n",
        "        word_id = 0\n",
        "        # word2id_dict,id2word_dict, wordId_frequency_dict 초기화\n",
        "        for per_word, per_count in word_freq.items():\n",
        "            if per_count < self.min_count:  # 미니멈 단어 빈도수에 못 미치는 단어들 제거\n",
        "                self.word_count_sum -= per_count\n",
        "                continue\n",
        "            self.id2word_dict[word_id] = per_word\n",
        "            self.word2id_dict[per_word] = word_id\n",
        "            self.wordId_frequency_dict[word_id] = per_count\n",
        "            word_id += 1\n",
        "        self.word_count = len(self.word2id_dict)\n",
        "\n",
        "    def _init_sample_table(self):\n",
        "        sample_table_size = 1e8\n",
        "        pow_frequency = np.array(list(self.wordId_frequency_dict.values())) ** 0.75  # 단어 등장 빈도수 3/4\n",
        "        word_pow_sum = sum(pow_frequency)  # 모든 단어의 총 단어 빈도\n",
        "        ratio_array = pow_frequency / word_pow_sum  # 단어 빈도수 비율\n",
        "        word_count_list = np.round(ratio_array * sample_table_size)\n",
        "        for word_index, word_freq in enumerate(word_count_list):\n",
        "            self.sample_table += [word_index] * int(word_freq)  # 목록 생성, 내용은 각 단어의 ID, 목록의 각 ID는 여러 번 반복되며, 반복 횟수는 매핑 후 단어 빈도입니다.\n",
        "        self.sample_table = np.array(self.sample_table)\n",
        "\n",
        "    # 미니 배치 크기의 양의 샘플 쌍 (w, v)을 얻습니다. \n",
        "    # 여기서 w는 대상 단어 ID이고 v는 컨텍스트에있는 단어의 ID입니다. \n",
        "    # 컨텍스트 단계 크기는 window_size, 즉 2c = 2 * window_size입니다.\n",
        "    def get_batch_pairs(self, batch_size, window_size):\n",
        "        while len(self.word_pairs_queue) < batch_size:\n",
        "            for _ in range(10000):  # 先加入10000条，减少循环调用次数\n",
        "                self.input_file = open(self.input_file_name, encoding=\"utf-8\")\n",
        "                sentence = self.input_file.readline()\n",
        "                if sentence is None or sentence == '':\n",
        "                    continue\n",
        "                wordId_list = []  # 一句中的所有word 对应的 id\n",
        "                for word in sentence.strip().split(' '):\n",
        "                    try:\n",
        "                        word_id = self.word2id_dict[word]\n",
        "                        wordId_list.append(word_id)\n",
        "                    except:\n",
        "                        continue\n",
        "                # 寻找正采样对 (w,v) 加入正采样队列\n",
        "                for i, wordId_w in enumerate(wordId_list):\n",
        "                    for j, wordId_v in enumerate(wordId_list[max(i - window_size, 0):i + window_size + 1]):\n",
        "                        assert wordId_w < self.word_count\n",
        "                        assert wordId_v < self.word_count\n",
        "                        if i == j:  # 上下文=中心词 跳过\n",
        "                            continue\n",
        "                        self.word_pairs_queue.append((wordId_w, wordId_v))\n",
        "        result_pairs = []  # 返回mini-batch大小的正采样对\n",
        "        for _ in range(batch_size):\n",
        "            result_pairs.append(self.word_pairs_queue.popleft())\n",
        "        return result_pairs\n",
        "\n",
        "    # 获取负采样 输入正采样对数组 positive_pairs，以及每个正采样对需要的负采样数 neg_count 从采样表抽取负采样词的id\n",
        "    # （假设数据够大，不考虑负采样=正采样的小概率情况）\n",
        "    def get_negative_sampling(self, positive_pairs, neg_count):\n",
        "        neg_v = np.random.choice(self.sample_table, size=(len(positive_pairs), neg_count)).tolist()\n",
        "        return neg_v\n",
        "\n",
        "    # 估计数据中正采样对数，用于设定batch\n",
        "    def evaluate_pairs_count(self, window_size):\n",
        "        return self.word_count_sum * (2 * window_size - 1) - (self.sentence_count - 1) * (1 + window_size) * window_size"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "EAThlBDu7aU9",
        "outputId": "80320eb4-5bd4-40a4-817a-24f3db00e08e"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    input_data = InputData('./data.txt', 3)\n",
        "    input_data"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8a1f5bc51df1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-b435ead75d5b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_file_name, min_count)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_file_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 데이터 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_count\u001b[0m  \u001b[0;31m# 제거할 단어의 최소 빈도수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordId_frequency_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 단어 ID- 빈도수 dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not numpy.ndarray"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhCPxxXw4Aq4"
      },
      "source": [
        "## **3. Word2Vec - SGNS 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR7iNnA7z-pt",
        "outputId": "8acaaff4-6568-4eeb-99c7-663f4271d888"
      },
      "source": [
        "# Word2Vec\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# hyper parameters\n",
        "WINDOW_SIZE = 4  # window size\n",
        "BATCH_SIZE = 64  # mini-batch\n",
        "MIN_COUNT = 3  # 제거해야하는 저빈도 단어의 빈도\n",
        "EMB_DIMENSION = 100  # embedding dimension\n",
        "LR = 0.02  # 학습률\n",
        "NEG_COUNT = 4 \n",
        "\n",
        "class Word2Vec:\n",
        "    def __init__(self, input_file_name, output_file_name):\n",
        "        self.output_file_name = output_file_name\n",
        "        self.data = InputData(input_file_name, MIN_COUNT)\n",
        "        self.model = SkipGramModel(self.data.word_count, EMB_DIMENSION)\n",
        "        self.lr = LR\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "    def train(self):\n",
        "        print(\"SkipGram Training......\")\n",
        "        pairs_count = self.data.evaluate_pairs_count(WINDOW_SIZE)\n",
        "        print(\"pairs_count\", pairs_count)\n",
        "        batch_count = pairs_count / BATCH_SIZE\n",
        "        print(\"batch_count\", batch_count)\n",
        "        process_bar = tqdm(range(int(batch_count)))\n",
        "        for i in process_bar:\n",
        "            pos_pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE)\n",
        "            pos_w = [int(pair[0]) for pair in pos_pairs]\n",
        "            pos_v = [int(pair[1]) for pair in pos_pairs]\n",
        "            neg_v = self.data.get_negative_sampling(pos_pairs, NEG_COUNT)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.model.forward(pos_w, pos_v, neg_v)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if i * BATCH_SIZE % 100000 == 0:\n",
        "                self.lr = self.lr * (1.0 - 1.0 * i / batch_count)\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group['lr'] = self.lr\n",
        "\n",
        "        self.model.save_embedding(self.data.id2word_dict, self.output_file_name)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    w2v = Word2Vec(input_file_name='./data.txt', output_file_name=\"word_embedding.txt\")\n",
        "    w2v.train()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/56074 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Word Count is: 13394\n",
            "Word Count Sum is 655532\n",
            "Sentence Count is: 50000\n",
            "SkipGram Training......\n",
            "pairs_count 3588744\n",
            "batch_count 56074.125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 56074/56074 [01:44<00:00, 534.77it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUn6SK1Y06wD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}